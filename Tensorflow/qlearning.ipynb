{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "16361a04417910c713468912220de5b5c263f0dfec75b27f35180881c9ffda18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, time, random, numpy as np\n",
    "from IPython.display import clear_output\n",
    "from gym.envs.registration import register\n",
    "try:\n",
    "    register(\n",
    "        id='FrozenLakeNoSlip-v0',\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4', 'is_slippery' : False},\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.7, # optimum = 0.74 Note: look into this tomorrow.\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "envName = \"FrozenLakeNoSlip-v0\"\n",
    "env = gym.make(envName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.actionSize = env.action_space.n\n",
    "    \n",
    "    def getAction(self):\n",
    "        action = random.randint(0, self.actionSize - 1)\n",
    "        return action\n",
    "    \n",
    "\n",
    "\n",
    "class qAgent(Agent):\n",
    "    def __init__(self, env, discount, learningRate):\n",
    "        super().__init__(env)\n",
    "        self.stateSize = env.observation_space.n\n",
    "        self.makeqTable()\n",
    "        self.discount = discount\n",
    "        self.learningRate = learningRate\n",
    "        self.eps = 1.0\n",
    "\n",
    "    def makeqTable(self):\n",
    "        self.qTable = 1e-4*np.zeros([self.stateSize, self.actionSize])\n",
    "    \n",
    "    def train(self, experience):\n",
    "        state, action, nextState, reward, done = experience\n",
    "\n",
    "        qNext = self.qTable[nextState]\n",
    "        qNext = np.zeros([self.actionSize]) if done else qNext\n",
    "        qTarget = reward + self.discount * np.max(qNext)\n",
    "\n",
    "        qUpdate = qTarget - self.qTable[state, action]\n",
    "        self.qTable[state, action] += self.learningRate * qUpdate\n",
    "        if done:\n",
    "            self.eps *= .997\n",
    "        \n",
    "\n",
    "    def getAction(self, state):\n",
    "        qState = self.qTable[state]\n",
    "        action = super().getAction() \n",
    "        greedy = np.argmax(qState)\n",
    "        return action if random.random() < self.eps else greedy\n",
    "agent = qAgent(env, .97, .01)\n",
    "totRewards = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "state: 15 eps: 0.007904966614247262 reward 1232.0\n  (Right)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n[[5.93907863e-02 8.46996042e-01 1.63475762e-03 6.84043269e-02]\n [4.58141047e-02 0.00000000e+00 3.93928849e-07 6.36587536e-05]\n [1.60664228e-04 0.00000000e+00 0.00000000e+00 6.12893166e-11]\n [2.28910970e-13 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [7.17773979e-02 8.80701108e-01 0.00000000e+00 4.23422024e-02]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.07644184e-06]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [9.02502896e-02 0.00000000e+00 9.11218751e-01 6.98405557e-02]\n [7.23383302e-02 9.40553125e-01 6.53841138e-02 0.00000000e+00]\n [1.04070982e-02 3.84225764e-01 0.00000000e+00 6.96636351e-09]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 1.70425407e-01 9.69945478e-01 8.50668259e-02]\n [1.49700907e-01 2.13710013e-01 9.99995849e-01 5.41944599e-02]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for _ in range(300):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.getAction(state)\n",
    "        nextState, reward, done, info = env.step(action)\n",
    "        agent.train((state, action, nextState, reward, done))\n",
    "        state = nextState\n",
    "        print(\"state:\",state,\"eps:\", agent.eps, \"reward\", totRewards)\n",
    "        totRewards += reward\n",
    "        env.render()\n",
    "        time.sleep(0.05)\n",
    "        print(agent.qTable)\n",
    "        clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}